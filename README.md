Self-Attention Mechanisms from Scratch
A complete implementation of self-attention mechanisms including trainable, non-trainable, and causal attention variants built from the ground up.

Overview

This project contains three core self-attention implementations:

Trainable Self-Attention: Standard attention with learnable Q, K, V projections
Non-trainable Self-Attention: Simplified version without learnable parameters
Causal Self-Attention: Masked attention for autoregressive tasks

All implementations are built from scratch to demonstrate the underlying mathematics clearly.
Features

Complete self-attention mechanisms with detailed comments
Causal masking for language modeling tasks
Educational focus with step-by-step computations
Comparison examples between different attention types
